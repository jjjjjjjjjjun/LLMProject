import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import numpy as np

# 1ï¸âƒ£ ëª¨ë¸ ë¡œë“œ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")

model_path = r"/LLMtuning/kogpt2_finetuned"
#model_path = "skt/kogpt2-base-v2"  # ëŒ€ì²´ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´ ì´ ì¤„ë¡œ ë³€ê²½
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    use_safetensors=True,
    dtype=torch.float32,
    output_attentions=True  # ëª¨ë¸ ë¡œë“œ ì‹œ output_attentions ê°•ì œ ì„¤ì • ì‹œë„
).to(device)

# íŒ¨ë”© í† í° ì„¤ì •
tokenizer.pad_token = tokenizer.eos_token
tokenizer.pad_token_id = 1
model.config.pad_token_id = 1

# ì–´í…ì…˜ ì¶œë ¥ ê°•ì œ í™œì„±í™”
model.config.output_attentions = True

print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ âœ…")
print("ëª¨ë¸ ì„¤ì •:", model.config)

# 2ï¸âƒ£ í† í° ë‹¨ìœ„ë¡œ ìƒì„± + ì–´í…ì…˜ ì¶”ì¶œ
def generate_with_attention(prompt, max_new_tokens=50):
    model.eval()
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)
    generated_ids = input_ids.clone()
    print(f"\n[í”„ë¡¬í”„íŠ¸]: {prompt}\n")
    all_attentions = []

    for step in range(max_new_tokens):
        with torch.no_grad():
            outputs = model(
                input_ids=generated_ids,
                attention_mask=torch.ones_like(generated_ids),
                output_attentions=True,
                use_cache=False
            )

        # ì–´í…ì…˜ ì¶œë ¥ í™•ì¸
        if outputs.attentions is None:
            print("ê²½ê³ : ì´ ëª¨ë¸ì€ ì–´í…ì…˜ ì¶œë ¥ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì–´í…ì…˜ ë¶„ì„ì„ ìƒëµí•©ë‹ˆë‹¤.")
            next_token_logits = outputs.logits[:, -1, :]
            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)
            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)
            if next_token_id.item() == tokenizer.eos_token_id:
                break
            continue

        next_token_logits = outputs.logits[:, -1, :]
        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)
        generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)

        # ì–´í…ì…˜ ì €ì¥ (ë§ˆì§€ë§‰ ë ˆì´ì–´, ëª¨ë“  í—¤ë“œ í‰ê· )
        last_attn = outputs.attentions[-1].mean(dim=1)  # (batch, seq_len, seq_len)
        all_attentions.append(last_attn[:, -1, :].squeeze(0).cpu().numpy())

        if next_token_id.item() == tokenizer.eos_token_id:
            break

    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    print(f"ğŸ“ ìƒì„± ë¬¸ì¥: {generated_text}\n")

    if all_attentions:
        tokens = tokenizer.convert_ids_to_tokens(generated_ids[0])
        importance = all_attentions[-1]
        top_idx = np.argsort(importance)[::-1][:5]
        print("ğŸ¯ ë§ˆì§€ë§‰ í† í° ê¸°ì¤€ ì–´í…ì…˜ ìƒìœ„ 5ê°œ í† í°:")
        for i in top_idx:
            print(f"   {tokens[i]}: {importance[i]:.4f}")

# 3ï¸âƒ£ ì¸í„°ë™í‹°ë¸Œ í…ŒìŠ¤íŠ¸
def interactive_test():
    print("\n[ì§€ì—­ ê¸°ë°˜ ë¬¸ì¥ ìƒì„± + ì–´í…ì…˜ ë¶„ì„] ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥")
    while True:
        region = input("\nì§€ì—­ ì…ë ¥ (ì˜ˆ: ì„œìš¸, ë¶€ì‚°, ì œì£¼ë„): ")
        if region.lower() == "exit":
            break
        prompt = f"{region}ì˜ ëŒ€í‘œ ê´€ê´‘ì§€ë¥¼ ì¶”ì²œí•´ì¤˜"
        generate_with_attention(prompt, max_new_tokens=50)

interactive_test()